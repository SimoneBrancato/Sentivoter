services:

  producer:
    build:
      context: producer/
      dockerfile: ./Dockerfile
    image: producer
    container_name: producer
    volumes:
      - ./yt_data/flattened_comments_data/:/comments_data/
      - ./yt_data/flattened_videos_data:/videos_data
    depends_on:
      logstash:
        condition: service_healthy
      elasticsearch:
        condition: service_started
    command: sh -c "sleep 90s ; python3 -u script.py"   

  
  logstash:
    build: 
      context: logstash/
      dockerfile: ./Dockerfile
    container_name: logstash
    environment:
      XPACK_MONITORING_ENABLED: "false"
      pipeline.ecs_compatibility: "disabled"
      LS_JAVA_OPTS: "-Xms1g -Xmx1g"
    ports:
      - 9700:9700
    depends_on:
      - kafka
      - kafka-ui
      - elasticsearch
    healthcheck:
      test: ["CMD-SHELL", "curl --silent --fail http://localhost:9700 || exit 1"]
      interval: 30s
      timeout: 30s
      retries: 10

  kafka:
    image: bitnami/kafka:3.7.0
    container_name: kafka
    ports:
      - 9092:9092
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:2181
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_KRAFT_CLUSTER_ID=MkU3OEVBNTcwNTJENDM2Qk
      - KAFKA_MESSAGE_MAX_BYTES=10485760
    entrypoint: /bin/bash
    command: -c "rm -rf /bitnami/kafka/data/* && /opt/bitnami/scripts/kafka/entrypoint.sh /opt/bitnami/scripts/kafka/run.sh"

  init-kafka:
    image: bitnami/kafka:3.7.0
    container_name: init-kafka
    depends_on:
      - kafka
      - kafka-ui
      - elasticsearch
    entrypoint: ['/bin/sh', '-c']
    command: |
      "
      echo 'Creating Kafka topics.'
      kafka-topics.sh --create --topic yt_sentivoter_videos --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1 --if-not-exists
      kafka-topics.sh --create --topic yt_sentivoter_comments --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1 --if-not-exists
      kafka-topics.sh --create --topic fb_sentivoter_posts --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1 --if-not-exists
      kafka-topics.sh --create --topic fb_sentivoter_comments --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1 --if-not-exists
      
      echo 'Successfully updated topic list:'
      kafka-topics.sh --list --bootstrap-server kafka:9092
      "

  kafka-ui:
    image: provectuslabs/kafka-ui:v0.7.2
    container_name: kafka-ui
    ports:
      - 8080:8080
    depends_on:
      - kafka
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=PLAINTEXT://kafka:9092

  fb_spark:
    build:
      context: fb_spark/
      dockerfile: ./Dockerfile
    container_name: fb_spark
    volumes:
      - ./fb_spark/:/spark
      - spark_volume_1:/opt/spark/jars
    deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: 1
                capabilities: [gpu] 
    command: > 
      /opt/spark/bin/spark-submit 
        --driver-memory 2g 
        --executor-memory 2g
        --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp -XX:+UseG1GC"
        --conf spark.executor.extraJavaOptions="-XX:+UseG1GC"
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.elasticsearch:elasticsearch-spark-30_2.12:8.13.4  
        /spark/script.py
    depends_on:
      producer:
        condition: service_completed_successfully
      init-kafka:
        condition: service_completed_successfully
      elasticsearch:
        condition: service_started

    yt_spark_1:
      build:
        context: yt_spark_1/
        dockerfile: ./Dockerfile
      container_name: yt_spark_1
      volumes:
        - ./yt_spark_1/:/spark
        - spark_volume_2:/opt/spark/jars
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: 1
                capabilities: [gpu] 
      command: > 
        /opt/spark/bin/spark-submit 
          --driver-memory 2g 
          --executor-memory 2g
          --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp -XX:+UseG1GC"
          --conf spark.executor.extraJavaOptions="-XX:+UseG1GC"
          --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.elasticsearch:elasticsearch-spark-30_2.12:8.13.4  
          /spark/script.py
      restart: on-failure
      depends_on:
        init-kafka:
          condition: service_completed_successfully
        elasticsearch:
          condition: service_started

  yt_spark_2:
    build:
      context: yt_spark_2/
      dockerfile: ./Dockerfile
    container_name: yt_spark_2
    volumes:
      - ./yt_spark_2/:/spark
      - spark_volume_3:/opt/spark/jars
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu] 
    command: >
      /opt/spark/bin/spark-submit 
      --driver-memory 3g 
      --executor-memory 3g 
      --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp -XX:+UseG1GC"
      --conf spark.executor.extraJavaOptions="-XX:+UseG1GC"
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.elasticsearch:elasticsearch-spark-30_2.12:8.13.4
      /spark/script.py
    restart: on-failure
    depends_on:
      init-kafka:
        condition: service_completed_successfully
      elasticsearch:
        condition: service_started

  elasticsearch:                                        
    image: docker.elastic.co/elasticsearch/elasticsearch:8.13.4
    container_name: elasticsearch
    volumes:
      - es_data:/usr/share/elasticsearch/data                  
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - http.port=9200
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
    ports:
      - 9200:9200
    deploy:
      resources:
        limits:
          memory: 4GB

  kibana:
    image: docker.elastic.co/kibana/kibana:8.7.1
    container_name: kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    volumes:
      - ./kibana/config:/usr/share/kibana/config
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch

  init-kibana:
    image: curlimages/curl:8.8.0
    container_name: init-kibana
    volumes:
      - ./kibana/config/data/export.ndjson:/export.ndjson
      - ./kibana/config/data/load-kibana.sh:/usr/share/kibana/load-kibana.sh
    command: ["/bin/sh", "/usr/share/kibana/load-kibana.sh"]
    depends_on:
      - kibana

  mysql:
    image: mysql:8.0
    container_name: mysql_database
    environment:
      MYSQL_DATABASE: elections
      MYSQL_ROOT_PASSWORD: root
      MYSQL_USER: user                    
      MYSQL_PASSWORD: user
    volumes:
      - ./mysql/dump.sql:/dump.sql
      - ./mysql/custom-entrypoint.sh:/custom-entrypoint.sh
      - db_data:/var/lib/mysql
    ports:
      - 3308:3306
    entrypoint: /custom-entrypoint.sh

volumes:
  spark_volume_1:
  spark_volume_2:
  spark_volume_3:
  es_data:
  db_data: